You will need the following files

SpeedDatingData.csv  and Speed Dating Data Key.doc  

NoSQL exercise witih Neo4J and MongoDB

Clean Up Docker (if you want to)
Remove all unused containers, networks, images (both dangling and unreferenced), and optionally,
volumes.
$ docker system prune
# in my case the above didn’t remove the images for some reason
$ docker image prune

NEO4J SECTION

Install and Access Neo4J
# Obtain the image
$ docker pull neo4j
# construct your host directories
$ mkdir /nosql
$ mkdir /nosql/neo4j
$ mkdir /nosql/neo4j/data
$ mkdir /nosql/neo4j/import
# Start up the container - Use the default ports below if there is a host conflict change them
accordingly.
# Note Windows user must precede the host directory with C:
$ docker run -p 7474:7474 -p 7687:7687 -d --restart unless-stopped -v /nosql/neo4j/data:/data
-v /nosql/neo4j/import:/import neo4j
# Get to the neo4j browser interface
Browser - http://localhost:7474
Default login: neo4j pw: neo4j
The system then requires a new password - Select another password - enter it and remember it!

Cypher is the query language used by neo4.


Recommend your select “Try Neo4j with live data” to see what it can do. It is easy to step
through it. Understand the basics of the cypher language.
The web interface uses the neo4j cypher language -
https://neo4j.com/docs/cypher-refcard/current/
# Load the data
Download the csv file (speeddatingReduced.csv) and put in the /nosql/neo4j/import directoryStudy the explanation file (speeddatingDesc.txt)
In the neo4J browser… -

Below is just an example. Add the additional fields in the
columns. It assumes a string unless changed.
You can load two ways - use the column name - row.<Column Name> or use a index row[0]
refers to the first column etc. The word row can be anything as long as you are consistent.

The following consists of two calls that can be combined in the browser command window.  The first call (LOAD ...) reads in the csv file from the filesystem in the container.  The second calls (MERGE ...) creates the nodes and relationships.  There are more columns that listed here but this gives you a good start...

LOAD CSV WITH HEADERS FROM "file:///SpeedDatingData.csv" AS row
WITH row WHERE NOT row.age IS null AND NOT row.iid IS null AND NOT row.pid IS null AND NOT row.age_o IS null and NOT row.race IS null and NOT row.race_o IS null and NOT row.match IS null and NOT row.int_corr IS null and NOT row.gender is  null
MERGE(p1 :Person {id:row.iid,age:toInteger(row.age), race:toInteger(row.race) }) MERGE( p2: Person {id:row.pid, age:toInteger(row.age_o), race:toInteger(row.race_o)}) MERGE((p1) - [:Date {match: row.match, int_corr: row.int_corr}] -> (p2)) SET p1.gender = toInteger(row.gender)

If you make a mistake, you can clear the database with
neo4j$ MATCH (n) DETACH DELETE n

From this load you can answer the following questions using cypher language in the neo4j browser window
Once loaded answer the following questions: Submit the answers in a word document and you
must show your cypher call(s) for each one. Each may require more than one cypher call.
1) Provide the schema
     db.schema.visualization())
2) Determine the ratio of Female to Males
3) Determining the % of dates that were matches (both selected eachother) 
   Select columns that would prove useful in predicting a match.
   Explain your reasoning.

Be careful with the logic - many participants go on multiple dates - you must figure out the correct way to uniquely identify individuals.  Enter these answers in the notebook with the query.

Now that you have the basics.  Go back to the Speed Dating Data Key.doc and select additional columns that you think would be useful in predicting a match.  Then setup supervised learning to create a model that predicts a match.  There isn't a lot of data so see what performance you can achieve.  You are free to use any model you like.


MONGO DB SECTION

Install and access MongoDB
Get the docker image
$ docker pull mongo:latest
Create host directories
$ mkdir /nosql/mongo
$ mkdir /nosql/mongo/import
$ mkdir /nosql/mongo/dataCopy in speeddatingReduced.csv to the import directory
Start up the mongo container
# Note Windows users must insert the drive letter prior to the local directory (eg. C:/)
$ docker run -d --restart unless-stopped -v c:/nosql/mongo/data:/data/db -v
c:/nosql/mongo/import:/import -p 27017:27017 mongo
Get the mongo container id
$ docker ps
Copy the Container ID for mongo <Container ID>
Go Inside the container
$ docker exec -it -u0 <Container ID> bash
Import csv file
$ mongoimport --type csv -d speeddating -c events --headerline --drop /import/SpeedDatingData.csv
#unless certain chars are removed, it will produce errors.  So the file speeddatingReduced.csv is available or you can try to clean it up youself.

Run Command Line Access to Mongo
root@<Container ID>/# mongosh  - this provides a basic interface to query the database.
1)  list databases
> show databases
2)  Use speeddating database
>  use speeddating
3) show collections inside of the speeddating database
> show collections
4)  show one of the collection documents
> db.events.findOne( )
5)  find all the matches

Enter these answers in the notebook with the query string.

Now query the features you need to build a prediction model for matches.

Finally, at the end of the notebook enter a section that contrast the two databases.  Note the advantages and disadvantages of each?  What type of situation is each best suited for?

Summary:
Create two notebooks:neo4jdating.ipynb and mongodating.ipynb.  Place all the answers in the notebooks with the final section in the mongodating.ipynb with the contrast observations between the two.  Although you are free to do it, it is not required to create a working docker image and place it into you docker hub repository.

